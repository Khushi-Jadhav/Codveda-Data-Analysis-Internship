# -*- coding: utf-8 -*-
"""IrisCode.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FyCZlujUq7UoVig06E5M0vtMcDvQZ8Dt
"""

# Task 1: Data Cleaning and Preprocessing
# Step 1: Import necessary libraries
import pandas as pd

# Step 2: Load the dataset
df = pd.read_csv("/content/drive/MyDrive/Data Set For Task/1) iris.csv")

# Step 3: Display the first few rows of the dataset
print("First 5 rows:\n", df.head())

# Step 4: Basic info of the dataset
print("\nDataset Info:")
print(df.info())

# Step 5: Check for missing values
print("\nMissing values in each column:")
print(df.isnull().sum())

# Optional: Fill or drop missing values
# Example: Fill missing numeric columns with mean
df.fillna(df.mean(numeric_only=True), inplace=True)

# Step 6: Remove duplicate rows
initial_shape = df.shape
df.drop_duplicates(inplace=True)
print(f"\nRemoved {initial_shape[0] - df.shape[0]} duplicate rows.")

# Step 7: Standardize column names (if inconsistent)
df.columns = df.columns.str.strip().str.lower().str.replace(" ", "_")
print("\nUpdated column names:")
print(df.columns)

# Step 8: Save cleaned dataset
df.to_csv("iris_cleaned.csv", index=False)
print("\nCleaned dataset saved as 'iris_cleaned.csv'")



# Task 2: Exploratory Data Analysis (EDA)
# Step 1: Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Step 2: Load the cleaned dataset
df = pd.read_csv("iris_cleaned.csv")

# Step 3: Summary statistics
print("\nSummary Statistics:")
print(df.describe())

# Step 4: Check the class distribution
print("\nClass Distribution:")
print(df['species'].value_counts())

# Step 5: Correlation matrix (for numeric columns)
print("\nCorrelation Matrix:")
print(df.corr(numeric_only=True))

# Plot the heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap="coolwarm")
plt.title("Correlation Matrix")
plt.tight_layout()
plt.savefig("correlation_matrix.png")
plt.show()

# Step 6: Histograms for each numerical feature
df.hist(figsize=(10, 8), bins=20, color='skyblue')
plt.suptitle("Feature Distributions", fontsize=16)
plt.tight_layout()
plt.savefig("feature_histograms.png")
plt.show()

# Step 7: Boxplots to detect outliers
plt.figure(figsize=(10, 6))
sns.boxplot(data=df.select_dtypes(include='number'))
plt.title("Boxplot of Features")
plt.tight_layout()
plt.savefig("boxplots.png")
plt.show()

# Step 8: Scatter plot (Sepal Length vs Petal Length)
plt.figure(figsize=(8, 6))
sns.scatterplot(x="sepal_length", y="petal_length", hue="species", data=df)
plt.title("Sepal Length vs Petal Length")
plt.tight_layout()
plt.savefig("scatter_sepal_petal.png")
plt.show()



# ---- imports ----
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# optional: prettier plots
sns.set_style("whitegrid")

# ---- load cleaned iris ----
file_path = "iris_cleaned.csv"          # adjust path as needed
df = pd.read_csv(file_path)

print(df.head())



# Task 3: Basic Data Visualization
# Bar Plot – Species Counts
plt.figure(figsize=(6,4))
sns.countplot(x="species", data=df)
plt.title("Count of Each Iris Species")
plt.xlabel("Species")
plt.ylabel("Count")
plt.tight_layout()
plt.show()

# Line Chart – Sorted Sepal Lengths
plt.figure(figsize=(7,4))
(
    df.sort_values("sepal_length")
      .reset_index(drop=True)["sepal_length"]
      .plot(marker="o")
)
plt.title("Sepal Lengths (sorted)")
plt.ylabel("cm")
plt.xlabel("Sample Index (sorted)")
plt.tight_layout()
plt.show()

# Scatter Plot – Sepal vs Petal (colored by species)
plt.figure(figsize=(6,5))
sns.scatterplot(
    x="sepal_length",
    y="petal_length",
    hue="species",
    data=df,
    s=70,
    edgecolor="black"
)
plt.title("Sepal Length vs Petal Length")
plt.tight_layout()
plt.show()



# Task 1:Linear Regression
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# ---- feature & target ----
X = df[["sepal_length"]]
y = df["petal_length"]

# ---- train / test split ----
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ---- model ----
linreg = LinearRegression()
linreg.fit(X_train, y_train)

# ---- evaluation ----
y_pred = linreg.predict(X_test)
print("Intercept :", linreg.intercept_)
print("Slope     :", linreg.coef_[0])
print("R²        :", r2_score(y_test, y_pred))
print("RMSE      :", np.sqrt(mean_squared_error(y_test, y_pred)))

# ---- plot regression line ----
plt.figure(figsize=(6,5))
sns.scatterplot(x="sepal_length", y="petal_length", data=df, alpha=0.7)
x_vals = np.array([df.sepal_length.min(), df.sepal_length.max()])
y_vals = linreg.intercept_ + linreg.coef_[0] * x_vals
plt.plot(x_vals, y_vals, linewidth=2)
plt.title("Linear Regression Fit")
plt.tight_layout()
plt.show()



# Task 2: Time Series Analysis
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose

# Sample Dataset Creation
# Create synthetic daily sales data (2 years)
date_range = pd.date_range(start='2022-01-01', end='2023-12-31', freq='D')
np.random.seed(42)

# Generate synthetic seasonal + trend + noise
trend = np.linspace(50, 100, len(date_range))  # gradual increase
seasonality = 10 * np.sin(2 * np.pi * date_range.dayofyear / 365.25)
noise = np.random.normal(0, 5, len(date_range))

sales = trend + seasonality + noise

# Build dataframe
df = pd.DataFrame({
    'date': date_range,
    'sales': sales
})
df.set_index('date', inplace=True)
print(df.head())

# Plot Time Series
plt.figure(figsize=(10, 5))
df['sales'].plot(title='Daily Sales Over Time')
plt.xlabel("Date")
plt.ylabel("Sales")
plt.tight_layout()
plt.show()

# Decompose the Series
 # Ensure index is datetime and frequency is set
df = df.asfreq('D')

# Decomposition (additive model)
decomposition = seasonal_decompose(df['sales'], model='additive', period=365)

# Plot decomposition
decomposition.plot()
plt.suptitle("Time Series Decomposition", fontsize=16)
plt.tight_layout()
plt.show()

# Moving Average Smoothing
# Add 30-day moving average
df['sales_rolling'] = df['sales'].rolling(window=30).mean()

plt.figure(figsize=(10, 5))
plt.plot(df['sales'], label='Original', alpha=0.5)
plt.plot(df['sales_rolling'], label='30-Day Moving Average', color='red')
plt.title("Sales with Moving Average Smoothing")
plt.xlabel("Date")
plt.ylabel("Sales")
plt.legend()
plt.tight_layout()
plt.show()



# Task 3:K‑Means Clustering
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# ---- choose numeric features ----
features = df[["sepal_length", "sepal_width", "petal_length", "petal_width"]]

# ---- scale ----
scaler = StandardScaler()
X_scaled = scaler.fit_transform(features)

# ---- elbow method to pick k ----
wcss = []
k_range = range(1, 11)
for k in k_range:
    km = KMeans(n_clusters=k, n_init=10, random_state=42)
    km.fit(X_scaled)
    wcss.append(km.inertia_)

plt.figure(figsize=(6,4))
plt.plot(k_range, wcss, marker="o")
plt.title("Elbow Method for Optimal k")
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Within‑Cluster Sum of Squares")
plt.tight_layout()
plt.show()

# ---- pick k = 3 (classic for iris) ----
k_opt = 3
kmeans = KMeans(n_clusters=k_opt, n_init=10, random_state=42)
cluster_labels = kmeans.fit_predict(X_scaled)

# ---- add cluster labels to df ----
df["cluster"] = cluster_labels

# ---- silhouette score ----
print("Silhouette score (k=3):", silhouette_score(X_scaled, cluster_labels))

# ---- 2‑D visualization using first 2 principal features
plt.figure(figsize=(6,5))
sns.scatterplot(
    x="petal_length",
    y="petal_width",
    hue="cluster",
    palette="viridis",
    data=df,
    s=70
)
plt.title("K‑Means Clusters (k=3) in Petal Space")
plt.tight_layout()
plt.show()



# Task 1: Predictive Modeling (Classification)
# Step 1: Import libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
import seaborn as sns
import matplotlib.pyplot as plt

# Step 2: Load dataset
df = pd.read_csv("iris_cleaned.csv")

# Step 3: Encode categorical target variable
df['species'] = df['species'].astype('category').cat.codes  # Setosa:0, Versicolor:1, Virginica:2

# Step 4: Features & Labels
X = df.drop('species', axis=1)
y = df['species']

# Step 5: Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 6: Feature Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Function to evaluate and print model performance
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    print(f"Model: {model.__class__.__name__}")
    print(classification_report(y_test, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print("\n" + "-"*50 + "\n")

# Logistic Regression
lr = LogisticRegression()
lr.fit(X_train_scaled, y_train)
evaluate_model(lr, X_test_scaled, y_test)

# Decision Tree
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)
evaluate_model(dt, X_test, y_test)

# Random Forest
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)
evaluate_model(rf, X_test, y_test)

# Grid Search for Random Forest
param_grid = {
    'n_estimators': [10, 50, 100],
    'max_depth': [2, 4, 6, None],
    'min_samples_split': [2, 5]
}

grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')
grid.fit(X_train, y_train)

print("Best Parameters:", grid.best_params_)
evaluate_model(grid.best_estimator_, X_test, y_test)



# Task 3: NLP – Sentiment Analysis
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from textblob import TextBlob
import nltk
from nltk.corpus import stopwords
import re

#Sample Dataset Creation
data = {
    "review": [
        "I love this product! It's amazing.",
        "Terrible service, very disappointed.",
        "Great value for money.",
        "Worst experience ever.",
        "Absolutely fantastic quality!",
        "It was okay, not the best.",
        "Will never buy this again."
    ]
}
df = pd.DataFrame(data)

# Preprocessing Function
nltk.download("stopwords")
stop_words = set(stopwords.words("english"))

def clean_text(text):
    text = text.lower()
    text = re.sub(r"[^a-zA-Z\s]", "", text)
    tokens = text.split()
    tokens = [t for t in tokens if t not in stop_words]
    return " ".join(tokens)

df["cleaned"] = df["review"].apply(clean_text)

# Sentiment with TextBlob
def get_sentiment(text):
    return TextBlob(text).sentiment.polarity

df["polarity"] = df["cleaned"].apply(get_sentiment)

def sentiment_label(p):
    if p > 0:
        return "Positive"
    elif p < 0:
        return "Negative"
    else:
        return "Neutral"

df["sentiment"] = df["polarity"].apply(sentiment_label)
print(df[["review", "sentiment"]])

# WordCloud & Distribution
# Word Cloud
all_text = " ".join(df["cleaned"])
wc = WordCloud(background_color="white").generate(all_text)

plt.figure(figsize=(6, 4))
plt.imshow(wc, interpolation="bilinear")
plt.axis("off")
plt.title("Word Cloud of Reviews")
plt.tight_layout()
plt.show()

# Sentiment distribution
df["sentiment"].value_counts().plot(kind="bar", color=["green", "red", "gray"])
plt.title("Sentiment Distribution")
plt.xlabel("Sentiment")
plt.ylabel("Count")
plt.tight_layout()
plt.show()



